{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34be49f-c4ad-436e-80dc-256b629e7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b448c6-f388-4f2d-8300-d7bdde4cb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.utils import to_categorical \n",
    "import pickle \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407b2a09-4046-4393-952f-d539fc036af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c726f0f-88f9-4a7a-874c-34ac78de501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         Dataline            Play  PlayerLinenumber ActSceneLine  \\\n",
      "0              1        Henry IV               NaN          NaN   \n",
      "1              2        Henry IV               NaN          NaN   \n",
      "2              3        Henry IV               NaN          NaN   \n",
      "3              4        Henry IV               1.0        1.1.1   \n",
      "4              5        Henry IV               1.0        1.1.2   \n",
      "...          ...             ...               ...          ...   \n",
      "111391    111392  A Winters Tale              38.0      5.3.180   \n",
      "111392    111393  A Winters Tale              38.0      5.3.181   \n",
      "111393    111394  A Winters Tale              38.0      5.3.182   \n",
      "111394    111395  A Winters Tale              38.0      5.3.183   \n",
      "111395    111396  A Winters Tale              38.0          NaN   \n",
      "\n",
      "               Player                                         PlayerLine  \n",
      "0                 NaN                                              ACT I  \n",
      "1                 NaN                       SCENE I. London. The palace.  \n",
      "2                 NaN  Enter KING HENRY, LORD JOHN OF LANCASTER, the ...  \n",
      "3       KING HENRY IV             So shaken as we are, so wan with care,  \n",
      "4       KING HENRY IV         Find we a time for frighted peace to pant,  \n",
      "...               ...                                                ...  \n",
      "111391        LEONTES         Lead us from hence, where we may leisurely  \n",
      "111392        LEONTES              Each one demand an answer to his part  \n",
      "111393        LEONTES     Perform'd in this wide gap of time since first  \n",
      "111394        LEONTES             We were dissever'd: hastily lead away.  \n",
      "111395        LEONTES                                             Exeunt  \n",
      "\n",
      "[111396 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Shakespeare_data.csv')\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a816b46e-93ac-45e6-99be-ffb320321efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACT I', 'SCENE I. London. The palace.', 'Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others', 'So shaken as we are, so wan with care,', 'Find we a time for frighted peace to pant,']\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "for i in data['PlayerLine']:\n",
    "    text.append(i)\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f7871a-99a3-4297-be7b-53ae491403b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['act i',\n",
       " 'scene i london the palace',\n",
       " 'enter king henry lord john of lancaster the earl of westmoreland sir walter blunt and others',\n",
       " 'so shaken as we are so wan with care',\n",
       " 'find we a time for frighted peace to pant']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    pattern = re.compile('[^a-zA-z0-9\\s]')\n",
    "    text = re.sub(pattern,'',text)\n",
    "\n",
    "    pattern = re.compile('/d+')\n",
    "    text = re.sub(pattern,'',text)\n",
    "\n",
    "    text = text.lower()\n",
    "    return text \n",
    "\n",
    "texts = []\n",
    "for t in text:\n",
    "    new_text = clean_text(t)\n",
    "    texts.append(new_text)\n",
    "\n",
    "texts[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41407bff-dffb-4f01-873d-44c21796e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts -->  act i\n",
      "Embedding -->  [455, 4]\n",
      "Maximum Sequence Length -->> 54\n",
      "Text Sequence -->>\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 455   4]\n",
      "Text Sequence Shape -->> (10000, 54)\n"
     ]
    }
   ],
   "source": [
    "texts = texts[:10000]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "print('texts --> ', texts[0])\n",
    "print('Embedding --> ', text_sequences[0])\n",
    "\n",
    "max_sequence_len = max([len(x) for x in text_sequences])\n",
    "text_sequences = pad_sequences(text_sequences, maxlen = max_sequence_len, padding = 'pre')\n",
    "\n",
    "print('Maximum Sequence Length -->>',max_sequence_len) \n",
    "print('Text Sequence -->>\\n',text_sequences[0]) \n",
    "print('Text Sequence Shape -->>',text_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155c74ab-073e-4621-b073-180d3d5cd7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 455]\n",
      "First output  4\n",
      "Total number of words :  7870\n",
      "Input shape -->  (10000, 53)\n",
      "Output shape -->  (10000, 7870)\n"
     ]
    }
   ],
   "source": [
    "X, Y = text_sequences[:,:-1], text_sequences[:,-1]\n",
    "print('First input ', X[0])\n",
    "print('First output ', Y[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "total_words = len(word_index) + 1\n",
    "print('Total number of words : ', total_words)\n",
    "\n",
    "Y = to_categorical(Y, num_classes=total_words)\n",
    "\n",
    "print('Input shape --> ', X.shape)\n",
    "print('Output shape --> ', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ce2fba-ef63-423e-9717-825a0bc242e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"LSTM_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(name=\"LSTM_Model\")\n",
    "\n",
    "# adding embedding\n",
    "model.add(Embedding(total_words,\n",
    "                   max_sequence_len-1,\n",
    "                   input_length=max_sequence_len-1))\n",
    "\n",
    "# adding a LSTM layer\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# adding the final output with activation function of softmax\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# printing model summary \n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f5779a-f5aa-471b-a54f-8c1d4dea056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 224ms/step - accuracy: 0.0099 - loss: 8.1875\n",
      "Epoch 2/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 222ms/step - accuracy: 0.0150 - loss: 7.3694\n",
      "Epoch 3/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 232ms/step - accuracy: 0.0260 - loss: 7.1221\n",
      "Epoch 4/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 228ms/step - accuracy: 0.0261 - loss: 6.8438\n",
      "Epoch 5/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 232ms/step - accuracy: 0.0299 - loss: 6.6272\n",
      "Epoch 6/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 226ms/step - accuracy: 0.0344 - loss: 6.3351\n",
      "Epoch 7/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m876s\u001b[0m 3s/step - accuracy: 0.0447 - loss: 5.9818\n",
      "Epoch 8/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 220ms/step - accuracy: 0.0604 - loss: 5.5330\n",
      "Epoch 9/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 231ms/step - accuracy: 0.0890 - loss: 5.1311\n",
      "Epoch 10/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 228ms/step - accuracy: 0.1200 - loss: 4.6644\n",
      "Epoch 11/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 224ms/step - accuracy: 0.1783 - loss: 4.1122\n",
      "Epoch 12/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 243ms/step - accuracy: 0.2480 - loss: 3.6310\n",
      "Epoch 13/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 228ms/step - accuracy: 0.3488 - loss: 3.1091\n",
      "Epoch 14/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 235ms/step - accuracy: 0.4347 - loss: 2.6447\n",
      "Epoch 15/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 230ms/step - accuracy: 0.5361 - loss: 2.2247\n",
      "Epoch 16/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 216ms/step - accuracy: 0.6040 - loss: 1.8657\n",
      "Epoch 17/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 207ms/step - accuracy: 0.6792 - loss: 1.5609\n",
      "Epoch 18/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 218ms/step - accuracy: 0.7367 - loss: 1.2900\n",
      "Epoch 19/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 208ms/step - accuracy: 0.7876 - loss: 1.0872\n",
      "Epoch 20/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 228ms/step - accuracy: 0.8235 - loss: 0.9246\n",
      "Epoch 21/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 215ms/step - accuracy: 0.8417 - loss: 0.8111\n",
      "Epoch 22/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 200ms/step - accuracy: 0.8636 - loss: 0.6993\n",
      "Epoch 23/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 215ms/step - accuracy: 0.8874 - loss: 0.5989\n",
      "Epoch 24/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 211ms/step - accuracy: 0.8880 - loss: 0.5689\n",
      "Epoch 25/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 216ms/step - accuracy: 0.8979 - loss: 0.5099\n",
      "Epoch 26/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 184ms/step - accuracy: 0.9088 - loss: 0.4586\n",
      "Epoch 27/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 192ms/step - accuracy: 0.9195 - loss: 0.4147\n",
      "Epoch 28/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 190ms/step - accuracy: 0.9301 - loss: 0.3760\n",
      "Epoch 29/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 188ms/step - accuracy: 0.9356 - loss: 0.3396\n",
      "Epoch 30/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 192ms/step - accuracy: 0.9324 - loss: 0.3400\n",
      "Epoch 31/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 190ms/step - accuracy: 0.9390 - loss: 0.3011\n",
      "Epoch 32/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 190ms/step - accuracy: 0.9378 - loss: 0.2979\n",
      "Epoch 33/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 4s/step - accuracy: 0.9413 - loss: 0.2851\n",
      "Epoch 34/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 174ms/step - accuracy: 0.9462 - loss: 0.2648\n",
      "Epoch 35/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 178ms/step - accuracy: 0.9468 - loss: 0.2540\n",
      "Epoch 36/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 180ms/step - accuracy: 0.9487 - loss: 0.2483\n",
      "Epoch 37/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 185ms/step - accuracy: 0.9458 - loss: 0.2531\n",
      "Epoch 38/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 179ms/step - accuracy: 0.9456 - loss: 0.2486\n",
      "Epoch 39/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 183ms/step - accuracy: 0.9465 - loss: 0.2513\n",
      "Epoch 40/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 181ms/step - accuracy: 0.9504 - loss: 0.2243\n",
      "Epoch 41/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 190ms/step - accuracy: 0.9531 - loss: 0.2032\n",
      "Epoch 42/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 190ms/step - accuracy: 0.9481 - loss: 0.2357\n",
      "Epoch 43/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 186ms/step - accuracy: 0.9517 - loss: 0.2090\n",
      "Epoch 44/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 178ms/step - accuracy: 0.9522 - loss: 0.2213\n",
      "Epoch 45/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 549ms/step - accuracy: 0.9531 - loss: 0.2107\n",
      "Epoch 46/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - accuracy: 0.9532 - loss: 0.2025\n",
      "Epoch 47/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 638ms/step - accuracy: 0.9497 - loss: 0.2062\n",
      "Epoch 48/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 285ms/step - accuracy: 0.9509 - loss: 0.1993\n",
      "Epoch 49/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 285ms/step - accuracy: 0.9507 - loss: 0.1998\n",
      "Epoch 50/50\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 286ms/step - accuracy: 0.9616 - loss: 0.1663\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model \n",
    "model.compile( \n",
    "\tloss=\"categorical_crossentropy\", \n",
    "\toptimizer='adam', \n",
    "\tmetrics=['accuracy'] \n",
    ") \n",
    "\n",
    "# Training the LSTM model \n",
    "history = model.fit(X, Y, \n",
    "\t\t\t\t\tepochs=50, \n",
    "\t\t\t\t\tverbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16e6331a-9a8a-4efa-8625-161dec5a3477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have seen this .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def autoCompletations(text, model): \n",
    "\t# Tokenization and Text vectorization \n",
    "\ttext_sequences = tokenizer.texts_to_sequences(text) \n",
    "\t# Pre-padding \n",
    "\ttesting = pad_sequences(text_sequences, maxlen = max_sequence_len-1, padding='pre') \n",
    "\t# Prediction \n",
    "\ty_pred_test = np.argmax(model.predict(testing,verbose=0)) \n",
    "\t\n",
    "\tpredicted_word = '' \n",
    "\tfor word, index in tokenizer.word_index.items(): \n",
    "\t\tif index == y_pred_test: \n",
    "\t\t\tpredicted_word = word \n",
    "\t\t\tbreak\n",
    "\ttext += \" \" + predicted_word + '.'\n",
    "\treturn text \n",
    "\t\n",
    "complete_sentence = autoCompletations('I have seen this', model) \n",
    "complete_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02550e5f-c0d7-4a22-b422-1994446bd008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have seen     '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(text, new_words): \n",
    "\tfor _ in range(new_words): \n",
    "\t\ttext = autoCompletations(text, model)[:-1] \n",
    "\treturn text \n",
    "\t\n",
    "generated_text = generate_text('I have seen', 5) \n",
    "generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c6365f-b9a8-430f-8797-3f4d6c25c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# saving the model \n",
    "model.save('sentence_completion.h5') \n",
    "\n",
    "# saving the tokenizer \n",
    "filename = 'tokenizer.pkl'\n",
    "pickle.dump(tokenizer, open(filename, 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
